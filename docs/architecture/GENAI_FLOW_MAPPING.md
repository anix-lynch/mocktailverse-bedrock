# ğŸ§  GenAI Architecture â†’ Mocktailverse Implementation

> **Your mental model mapped to actual deployed code**

---

## ğŸ¯ Quick Reality Check

### What's Actually Deployed (REAL)
```
mocktailverse-bedrock/
â”œâ”€â”€ âœ… frontend/app/page.tsx          (Single-page Next.js UI)
â”œâ”€â”€ âœ… lambdas/
â”‚   â”œâ”€â”€ agent/handler.py             (Bedrock Agent - fallback mode)
â”‚   â”œâ”€â”€ rag/handler.py               (RAG with Titan Text Lite)
â”‚   â”œâ”€â”€ search/handler.py            (DynamoDB vector search)
â”‚   â”œâ”€â”€ search_tool/handler.py       (Tool wrapper - not integrated)
â”‚   â”œâ”€â”€ embed/handler.py             (Titan Embeddings v2)
â”‚   â””â”€â”€ ingest/handler.py            (API fetch + enrichment)
â”œâ”€â”€ âœ… docs/                          (Architecture + guides)
â”œâ”€â”€ âœ… scripts/                       (Deployment scripts)
â”œâ”€â”€ âš ï¸  archive/, legacy/, lambda/    (Old code - creates confusion)
â””â”€â”€ âŒ terraform/, workflows/         (Infra managed manually or lost)
```

### What README Claims (IDEAL)
```
- Claude 3.5 for generation â†’ ACTUALLY: Titan Text Lite (cost optimization)
- Bedrock Agents with tools â†’ ACTUALLY: Fallback mode (AGENT_ID = None)
- OpenSearch vector DB      â†’ ACTUALLY: DynamoDB KNN fallback
- Step Functions workflows  â†’ ACTUALLY: Manual triggers or EventBridge schedules
- Clean folder structure    â†’ ACTUALLY: Mixed with legacy folders
```

**Gap:** MVP shipped with cost-optimized substitutions. Production-ready architecture exists in code comments but uses cheaper alternatives.

---

## The Flow

```
                USERS / PRODUCT                         ğŸ‘‰ You ask a question.
         (chat / search / generate / edit)                 (human input)
                           |
                           v
                    API Gateway / Backend              ğŸ‘‰ The request enters the system.
                   (auth / rate limit / logs)              (security + logging)
                           |
                           v
                    Prompt Orchestrator                ğŸ‘‰ A traffic cop decides the path.
               (system + user + tools)                    (routing brain)
                           |
        +------------------+------------------+
        |                                     |
        v                                     v

   RETRIEVAL PATH                        TOOL PATH        ğŸ‘‰ A smart helper checks files or calls helpers.
 (RAG pipeline)                    (function calling)       (search vs action)

        |                                     |
        v                                     v

Document Sources                      External Systems     ğŸ‘‰ Where info comes from.
(PDF / DB / S3 / Notion)          (CRM / Search / APIs)      (data + services)
        |                                     |
        v                                     v
 Ingestion + Chunking                   Tool Execution     ğŸ‘‰ Clean data or run tools.
(clean / split / embed)                     |                 (prepare info)
        |                                     |
        v                                     v
 Vector Database                         Tool Results       ğŸ‘‰ Store meaning / return results.
(Pinecone / FAISS)                           |
        |                                     |
        +---------------+---------------------+

                        Context Assembly      ğŸ‘‰ It gathers everything.
                  (top-k docs + tool outputs)    (builds backpack)
                        |
                        v
                  Final Prompt               ğŸ‘‰ Pack context for the model.
        (system + user + retrieved context)    (ready to think)
                        |
                        v
                     LLM Call               ğŸ‘‰ The big brain thinks.
          (GPT / Claude / Mistral / etc)
                        |
                        v
                   Model Response           ğŸ‘‰ Raw AI answer.
                        |
                        v
                  Post Processing           ğŸ‘‰ Clean + guard the output.
        (format / guardrails / validation)
                        |
                        v
                  User Experience           ğŸ‘‰ You see the result.
             (chat reply / design / text)
                        |
                        v
                 Telemetry + Feedback       ğŸ‘‰ The system remembers.
        (tokens / latency / thumbs / edits)    (signals)
                        |
                        v
                        Kafka               ğŸ‘‰ Events move downstream.
                        |
                        v
                 GenAI Observability        ğŸ‘‰ Everything is monitored.
          logs â†’ bronze â†’ silver â†’ gold
                        |
        +---------------+----------------+
        |                                |
        v                                v
 Quality Evaluation                 Cost + Latency        ğŸ‘‰ Was it good? Was it fast?
(accuracy / hallucination)        (token spend / p95)

        |
        v
 Prompt / Retrieval / Model Tuning          ğŸ‘‰ Improve the system.
        |                                     (optimize prompts + search)
        +----------------------+
                               |
                               v
                           Back to Top      ğŸ‘‰ Loop again.
```

---

## ğŸ¯ Component Mapping: Theory â†’ Implementation

### 1ï¸âƒ£ **USERS / PRODUCT** (Human Input)
**Theory:** User asks a question via chat/search interface

**Mocktailverse:**
- **Location:** `frontend/app/`
- **Files:**
  - `frontend/app/page.tsx` - Main UI (combined chat + search interface)
  - `frontend/app/components/` - Reusable UI components
  - `frontend/app/layout.tsx` - Root layout
  - `frontend/app/globals.css` - Global styles
- **Tech:** Next.js 14, TypeScript, Tailwind CSS
- **Deployed:** Vercel CDN at `gozeroshot.dev/mocktailverse`
- **Note:** Single-page app, not separate routes for chat/search

---

### 2ï¸âƒ£ **API GATEWAY / BACKEND** (Request Entry)
**Theory:** Security layer with auth, rate limiting, and logging

**Mocktailverse:**
- **Service:** AWS API Gateway
- **Endpoint:** `https://3m4c6fyw35.execute-api.us-west-2.amazonaws.com/prod`
- **Routes:**
  - `POST /search` â†’ `mocktailverse-search` Lambda
  - `POST /rag` â†’ `mocktailverse-rag` Lambda
  - `POST /agent/chat` â†’ `mocktailverse-agent` Lambda
- **Features:**
  - CORS enabled for `gozeroshot.dev`
  - CloudWatch logging
  - Throttling (default AWS limits)
- **Infrastructure:** `terraform/api_gateway.tf` (if exists, or configured manually)

---

### 3ï¸âƒ£ **PROMPT ORCHESTRATOR** (Routing Brain)
**Theory:** Traffic cop that decides retrieval vs tool calling path

**Mocktailverse:**
- **Lambda:** `mocktailverse-agent`
- **Location:** `lambdas/agent/handler.py`
- **Tech:** AWS Bedrock Agents + **Amazon Titan Text Lite** (FREE tier, not Claude)
- **Logic:**
  - Analyzes user intent (search vs generate vs variation)
  - Routes to DynamoDB search (no external tools configured yet)
  - Maintains session state for multi-turn conversations
  - **Fallback mode:** Direct Bedrock API call when Agent not configured
- **Deployment:** AWS Lambda (Python 3.11)
- **Current State:** `AGENT_ID = None` (using fallback, not full Bedrock Agent)

---

### 4ï¸âƒ£ **RETRIEVAL PATH** (RAG Pipeline)

#### **Document Sources**
**Theory:** Where raw data lives (PDF, DB, S3, Notion)

**Mocktailverse:**
- **S3 Buckets:**
  - `mocktailverse-raw-340752826866` - Raw cocktail recipes from APIs
  - `mocktailverse-processed-340752826866` - Cleaned/enriched data
- **DynamoDB Tables:**
  - `mocktailverse-cocktails` - Recipe metadata
  - `mocktailverse-metadata` - Embeddings + search index
- **External APIs:** TheCocktailDB API (initial data source)

#### **Ingestion + Chunking**
**Theory:** Clean, split, and prepare data for embedding

**Mocktailverse:**
- **Lambdas:**
  - `mocktailverse-fetch-cocktails` - Pulls from external API
  - `mocktailverse-ingest` - Cleans + enriches with Bedrock Claude
  - `mocktailverse-transform` - Prepares for embedding
- **Location:** `lambdas/ingest/handler.py`, `lambdas/transform/handler.py`
- **Process:**
  1. Fetch raw JSON from TheCocktailDB
  2. Use Claude 3.5 to extract metadata (flavor profile, tasting notes)
  3. Store in S3 + DynamoDB

#### **Ingestion + Chunking**
**Theory:** Clean, split, and embed data

**Mocktailverse:**
- **Lambda:** `mocktailverse-embed`
- **Location:** `lambdas/embed/handler.py`
- **Tech:** AWS Bedrock Titan Embeddings v2 (1536 dimensions)
- **Process:**
  1. Read from `mocktailverse-processed`
  2. Generate embeddings for each recipe
  3. Store in `mocktailverse-embeddings` S3 bucket
  4. Write metadata to DynamoDB `mocktailverse-metadata`

#### **Vector Database**
**Theory:** Stores semantic meaning (Pinecone/FAISS)

**Mocktailverse:**
- **Implementation:** DynamoDB with embeddings (cost-optimized, NO OpenSearch)
- **Table:** `mocktailverse-metadata`
- **Schema:**
  - `cocktail_id` (primary key)
  - `embedding` (1536-dim vector as binary)
  - `name`, `ingredients`, `flavor_profile`
- **Search:** KNN similarity search via Lambda logic (`lambdas/search/handler.py`)
- **Note:** Code has OpenSearch import but uses DynamoDB fallback (lines 22-49 in search handler)

---

### 5ï¸âƒ£ **TOOL PATH** (Function Calling)

#### **External Systems**
**Theory:** CRM, Search APIs, external services

**Mocktailverse:**
- **Lambda:** `mocktailverse-search-tool`
- **Location:** `lambdas/search_tool/handler.py`
- **Purpose:** Wrapper for DynamoDB search (intended for Bedrock Agent tool calling)
- **Current State:** âš ï¸ Defined but not yet integrated with Bedrock Agents

#### **Tool Execution**
**Theory:** Run functions and return results

**Mocktailverse:**
- **Status:** âš ï¸ Simplified implementation
- **Current:** Direct DynamoDB queries (not full tool orchestration)
- **Planned Tools:**
  - `search_cocktails(query)` - Finds recipes by natural language
  - `suggest_variation(base_cocktail)` - Generates creative variations
  - `get_tasting_notes(cocktail_id)` - Returns flavor analysis
- **Note:** MVP uses direct Lambda invocations, not Bedrock Agent action groups

---

### 6ï¸âƒ£ **CONTEXT ASSEMBLY** (Builds Backpack)
**Theory:** Gathers top-K docs + tool outputs

**Mocktailverse:**
- **Lambda:** `mocktailverse-rag`
- **Location:** `lambdas/rag/handler.py`
- **Process:**
  1. Receive user query
  2. Call `mocktailverse-search` to get top-5 relevant recipes
  3. Retrieve full metadata from DynamoDB
  4. Assemble context string with recipe details
  5. Add system prompt for grounding

---

### 7ï¸âƒ£ **FINAL PROMPT** (Ready to Think)
**Theory:** Pack context for the LLM

**Mocktailverse:**
- **Location:** Inside `lambdas/rag/handler.py` and `lambdas/agent/handler.py`
- **Structure:**
  ```python
  prompt = f"""
  System: You are an expert bartender AI. Answer ONLY from the provided context.
  
  Context:
  {retrieved_recipes}
  
  User Query: {user_question}
  
  Instructions: If context doesn't contain the answer, say "I don't know."
  """
  ```

---

### 8ï¸âƒ£ **LLM CALL** (Big Brain Thinks)
**Theory:** GPT, Claude, Mistral

**Mocktailverse:**
- **Service:** AWS Bedrock
- **Models:**
  - **Amazon Titan Text Lite** (`amazon.titan-text-lite-v1`) - Main generation âœ… FREE tier
  - **Titan Embeddings v2** (`amazon.titan-embed-text-v2:0`) - Vector generation
- **Why Titan:** Cost optimization for MVP (FREE tier, no access request needed)
- **Params:**
  - `temperature: 0.3` (low creativity for factual answers)
  - `max_tokens: 1024`
  - `top_p: 0.9`
- **Note:** README mentions Claude 3.5, but code uses Titan (see handler.py lines 18-24)

---

### 9ï¸âƒ£ **MODEL RESPONSE** (Raw AI Answer)
**Theory:** Unprocessed LLM output

**Mocktailverse:**
- **Format:** JSON response from Bedrock
- **Structure:**
  ```json
  {
    "completion": "Based on the recipes...",
    "stop_reason": "end_turn",
    "usage": {
      "input_tokens": 523,
      "output_tokens": 187
    }
  }
  ```

---

### ğŸ”Ÿ **POST PROCESSING** (Clean + Guard)
**Theory:** Format, validate, apply guardrails

**Mocktailverse:**
- **Location:** Lambda response handlers
- **Checks:**
  - âœ… Remove markdown artifacts
  - âœ… Validate JSON structure
  - âœ… Check for refusal ("I don't know")
  - âœ… Truncate if too long
- **Guardrails:** Bedrock Guardrails (content filtering)

---

### 1ï¸âƒ£1ï¸âƒ£ **USER EXPERIENCE** (You See Result)
**Theory:** Chat reply, formatted UI

**Mocktailverse:**
- **Frontend:** `frontend/app/page.tsx` (single-page app)
- **Components:** `frontend/app/components/`
- **Display:**
  - Search results with cocktail cards
  - Recipe details (ingredients, instructions)
  - Clean, minimal UI with Tailwind CSS
- **UX:** Responsive design, loading states, error handling
- **Note:** Combined search + display (not separate chat interface)

---

### 1ï¸âƒ£2ï¸âƒ£ **TELEMETRY + FEEDBACK** (System Remembers)
**Theory:** Token usage, latency, thumbs up/down

**Mocktailverse:**
- **Current Implementation:** âš ï¸ Basic CloudWatch logs
- **Metrics Available:**
  - Lambda duration (p95 latency)
  - Bedrock token usage (input/output)
  - Error rates
- **Missing (TODO):**
  - User feedback (thumbs up/down)
  - Explicit hallucination tracking
  - A/B testing framework

---

### 1ï¸âƒ£3ï¸âƒ£ **KAFKA** (Events Downstream)
**Theory:** Stream events for further processing

**Mocktailverse:**
- **Status:** âŒ Not implemented
- **Alternative:** EventBridge scheduled rules
- **Current Use:** Daily ingestion triggers (cron-like)

---

### 1ï¸âƒ£4ï¸âƒ£ **GENAI OBSERVABILITY** (Everything Monitored)
**Theory:** logs â†’ bronze â†’ silver â†’ gold

**Mocktailverse:**
- **Current:** CloudWatch Logs (raw logs only)
- **Status:** âš ï¸ Basic observability
- **Missing (TODO):**
  - Structured logging (bronze)
  - Aggregated metrics (silver)
  - Business KPIs (gold)
- **Future:** Integrate with DataDog or custom pipeline

---

### 1ï¸âƒ£5ï¸âƒ£ **QUALITY EVALUATION** (Was it Good?)
**Theory:** Accuracy, hallucination detection

**Mocktailverse:**
- **Current Strategy:** Retrieval-first grounding
- **Guardrails:**
  - Always fetch top-K before generation
  - "I don't know" refusal rule
  - Temperature â‰¤ 0.3 for factual queries
- **Target KPI:** Hallucination rate < 5%
- **Status:** âš ï¸ No automated evaluation yet

---

### 1ï¸âƒ£6ï¸âƒ£ **COST + LATENCY** (Was it Fast?)
**Theory:** Token spend, p95 latency

**Mocktailverse:**
- **Cost Tracking:**
  - CloudWatch tracks Bedrock API calls
  - Current: ~$1-2/month (MVP)
  - Target: < $0.01 per query
- **Latency:**
  - Target: p95 < 5s
  - Current: ~2-3s (search) / ~4-5s (RAG)
- **Monitoring:** CloudWatch Lambda metrics

---

### 1ï¸âƒ£7ï¸âƒ£ **PROMPT / RETRIEVAL / MODEL TUNING** (Improve System)
**Theory:** Optimize prompts, improve search

**Mocktailverse:**
- **Current:** Manual prompt engineering
- **Tuning Levers:**
  - `top-k` for retrieval (currently 5)
  - Context window size
  - Temperature / top_p
  - System prompt refinement
- **Future:** A/B testing, automated prompt optimization

---

## ğŸ¯ Interview Walkthrough Script

### **When interviewer asks: "Walk me through your GenAI architecture"**

> "Mocktailverse is a production RAG system showcasing serverless GenAI architecture on AWS.
>
> **Entry point:** Next.js frontend on Vercel â†’ AWS API Gateway â†’ Lambda handlers.
>
> **Data pipeline:** 
> 1. Ingest cocktail data from TheCocktailDB API
> 2. Enrich with Bedrock (extract flavor profiles, tasting notes)
> 3. Generate 1536-dim embeddings via Titan Embeddings v2
> 4. Store in DynamoDB with metadata
>
> **Search:** Custom KNN similarity search in DynamoDB (chose this over OpenSearch for costâ€”saves ~$100/month).
>
> **RAG flow:** User query â†’ embed â†’ KNN search (top-5) â†’ context assembly â†’ Bedrock generation â†’ grounded answer.
>
> **Cost optimization:** Using Titan Text Lite instead of Claude 3.5 for MVPâ€”FREE tier, no access request needed. Production would swap to Claude for better reasoning.
>
> **Guardrails:** Temperature 0.3, refusal logic ('I don't know'), retrieval-first to prevent hallucination.
>
> **Results:** ~$1-2/month runtime cost, p95 latency ~3-4s, fully serverless.
>
> **Tradeoffs I made:** DynamoDB vs OpenSearch (cost), Titan vs Claude (free tier), manual infra vs Terraform (speed to ship). Happy to discuss production hardening."

### **When they ask: "Why not OpenSearch/Claude/etc?"**

> "**Engineering tradeoff for MVP cost control:**
> - OpenSearch Serverless: $100-300/month minimum â†’ DynamoDB: $0.25/month with free tier
> - Claude 3.5 Sonnet: Requires access request + higher token cost â†’ Titan Text Lite: FREE, instant access
> - Bedrock Agents full setup: Requires IAM roles, action groups â†’ Fallback mode: Direct API calls work immediately
>
> **I prioritized:** Shipping a working demo with real GenAI patterns over maximal performance. In production, I'd upgrade based on usage metrics and budget."

---

## ğŸ“‚ ACTUAL File Reference Map

| Component | File Path | Status |
|-----------|-----------|--------|
| Frontend UI | `frontend/app/page.tsx` | âœ… Deployed |
| Frontend Components | `frontend/app/components/` | âœ… Deployed |
| Agent Orchestrator | `lambdas/agent/handler.py` | âœ… Deployed (fallback mode) |
| RAG Pipeline | `lambdas/rag/handler.py` | âœ… Deployed |
| Vector Search | `lambdas/search/handler.py` | âœ… Deployed |
| Search Tool | `lambdas/search_tool/handler.py` | âš ï¸ Defined, not integrated |
| Embedding Generation | `lambdas/embed/handler.py` | âœ… Deployed |
| Data Ingestion | `lambdas/ingest/handler.py` | âœ… Deployed |
| Infrastructure (Terraform) | `legacy/terraform/main.tf` | âš ï¸ Legacy folder (not active) |
| Architecture Docs | `docs/ARCHITECTURE.md` | âœ… Exists |
| API Specs | `docs/API_SPEC.md` | âš ï¸ Check if exists |
| GenAI Flow Map | `docs/GENAI_FLOW_MAPPING.md` | âœ… This file |

**Notes:**
- No active `terraform/` folder in root (infra likely deployed manually or via scripts)
- No `workflows/` folder (EventBridge scheduled via AWS console)
- `archive/`, `legacy/`, `lambda/` folders contain old/deprecated code

---

## âœ… Verified Deployed Resources (AWS Account 340752826866)

**Lambdas (8 functions running):**
```
mocktailverse-agent          (Python 3.11, last: 2025-11-25)
mocktailverse-rag            (Python 3.11, last: 2025-11-25)
mocktailverse-search         (Python 3.11, last: 2025-11-25)
mocktailverse-search-tool    (Python 3.11, last: 2025-11-25)
mocktailverse-embed          (Python 3.11, last: 2025-11-25)
mocktailverse-ingest         (Python 3.11, last: 2025-11-25)
mocktailverse-transform      (Python 3.11, last: 2025-11-15)
mocktailverse-fetch-cocktails(Python 3.9,  last: 2025-11-24)
```

**S3 Buckets (6 buckets):**
```
mocktailverse-raw-340752826866           (Raw data from API)
mocktailverse-processed-340752826866     (Enriched data)
mocktailverse-embeddings-340752826866    (Vector store)
mocktailverse-frontend-340752826866      (Static Next.js build)
mocktailverse-raw                        (Legacy)
mocktailverse-processed                  (Legacy)
```

**DynamoDB Tables (3 tables):**
```
mocktailverse-cocktails     (Recipe metadata)
mocktailverse-metadata      (Embeddings + search index)
mocktailverse-jobs          (Processing status)
```

**API Gateway:**
```
https://3m4c6fyw35.execute-api.us-west-2.amazonaws.com/prod
```

**Status:** âœ… All resources active and healthy (checked 2025-02-12)

---

## ğŸš€ Next Steps for Production-Grade System

**Missing components to add:**
1. **Kafka/EventBridge** - Event streaming for downstream analytics
2. **Bronze/Silver/Gold** - Data lake medallion architecture
3. **User feedback loop** - Thumbs up/down, explicit corrections
4. **A/B testing** - Prompt variation experiments
5. **Automated eval** - Hallucination detection, RAGAS metrics
6. **Cost dashboards** - Real-time token/$ tracking per endpoint

**Interview tip:** Acknowledge what's missing and explain the tradeoffs:
> "For MVP, I prioritized retrieval quality and cost optimization. The observability layer is basic CloudWatchâ€”in production, I'd add structured logging with a bronze/silver/gold pipeline and integrate RAGAS for automated evaluation."
